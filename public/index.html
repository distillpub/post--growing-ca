<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style>
  </style>
  <script src="twgl.min.js"></script>
  <script type="module" src="./ca.js"></script>
  <script type="module" src="./demo.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Growing Neural Cellular Automata",
      "description": "Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.",
      "authors": [
        {
          "author": "Alexander Mordvintsev",
          "authorURL": "https://znah.net/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Ettore Randazzo",
          "authorURL": "",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Eyvind Niklasson",
          "authorURL": "https://eyvind.me/",
          "affiliation": "Google",
          "affiliationURL": "https://ai.google/"
        },
        {
          "author": "Michael Levin",
          "authorURL": "http://www.drmichaellevin.org",
          "affiliation": "Allen Discovery Center at Tufts University",
          "affiliationURL": "http://allencenter.tufts.edu"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <style>
    figure {
      text-align: center;
      margin-bottom: 0.5em;
      margin-top: 0.5em;
    }
    figure img {
      max-width: 100%;
      width: unset;
    }
    video {
      max-width: 100%;
    }
    .colab-root {
      display: inline-block;
      background: rgba(255, 255, 255, 0.75);
      padding: 4px 8px;
      border-radius: 4px;
      font-size: 11px!important;
      text-decoration: none;
      color: #aaa;
      border: none;
      font-weight: 300;
      border: solid 1px rgba(0, 0, 0, 0.08);
      border-bottom-color: rgba(0, 0, 0, 0.15);
      text-transform: uppercase;
      line-height: 16px;
    }

   span.colab-span {
      background-image: url(images/colab.svg);
      background-repeat: no-repeat;
      background-size: 20px;
      background-position-y: 2px;
      display: inline-block;
      padding-left: 24px;
      border-radius: 4px;
      text-decoration: none;
    }

    a.colab-root:hover{
      color: #666;
      background: white;
      border-color: rgba(0, 0, 0, 0.2);
    }

    /* TOC */
    @media(max-width: 1000px){
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    } 
    
    @media (min-width: 1000px){
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1180px){
      d-contents {
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }
    
    d-contents nav ul li {
      margin-bottom: .25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
        text-decoration: none;
    }

    /* code blocks to margins */
    @media (min-width: 1600px) {
      d-code {
        margin-top: -10px;
        grid-column-start: 12;
        grid-column-end: 14; 
      }
    }
    /* so title is on one line */
    d-title h1, d-title p {
      grid-column: middle;
    }

  </style>
  <script>
  // hack to edit font size in code snippets. guaranteed a better way to do 
  // this, but I'm not a webdev
  window.onload = function() {
    setTimeout(() => { document.querySelectorAll("d-code").forEach(function(e) {e.shadowRoot.querySelector('#code-container').style.fontSize = "0.7em"}); }, 3000);
  }
  </script>
  <d-title>
    <h1>Growing Neural Cellular Automata</h1>
    <p>Differentiable Model of Morphogenesis</p>

<svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
    <symbol id="playIcon" viewBox="0 0 24 24"><path d="M8 5v14l11-7z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="pauseIcon" viewBox="0 0 24 24"><path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z"></path><path d="M0 0h24v24H0z" fill="none"></path></symbol>
    <symbol id="resetIcon" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"></path></symbol>
</svg>

<style>
#demo {
    font-size: 14px;
    user-select: none;
    grid-template-columns: auto;
    grid-template-rows: auto auto auto;
    grid-auto-flow: column;
    row-gap: 10px;
}

.hint a {
  color: inherit;
}

@media (min-width: 1000px) {
  #demo {
    grid-template-columns: 1fr 300px;
    grid-template-rows: auto auto;
  }
  #demo-controls {
    grid-row: 1/3;
  }
}

#demo-canvas {
    border: 1px solid lightgrey;
    image-rendering: pixelated;
    touch-action: none;
    width: 100%;
}

#demo-controls {
    line-height: 1em;
    display: grid;
    grid-template-columns: 120px auto;
    grid-template-rows: auto 60px 80px 75px 1fr;
    row-gap: 20px;
    overflow: hidden;
}

@media (min-width: 1000px){
  #demo-controls {
    grid-template-rows: auto 60px 80px 100px 1fr;
  }
}

#pattern-selector {
    grid-column: 1/3;
    display: grid;
    grid-template-columns: repeat(5, auto);
    justify-items: center;
}
@media (max-width: 1000px) and  (min-width: 500px) {
  #pattern-selector {
    grid-template-columns: repeat(10, auto);
  }
}

#pattern-selector * {
    width: 40px;
    height: 40px;
    background-image: url('images/emoji.png');
    cursor: pointer;
}
.icon {
    width: 30px; height: 30px;
    background: steelblue;
    fill: white;
    border-radius: 20px;
    padding: 5px;
    margin: 2px;
    cursor: pointer;
}
#model-selector {
    line-height: 1.4em;
}
#demo-tip{
    display: grid;
    grid-template-columns: 40px auto;
    align-items: center;
    column-gap: 10px;
    margin-bottom: 20px;
}
#pointer {
    width: 40px;
}
#status {
    font-size: 12px;
    color: rgba(0, 0, 0, 0.6);
    font-family: monospace;
}
#model-hints {
    color: rgba(0, 0, 0, 0.6);
    grid-column: 1/3;
}
#model-hints span {
    display: none;
}
.hint {
    color: rgba(0, 0, 0, 0.6);
    line-height: 1.4em;
    user-select: text;
}

input[type=range] {
  -webkit-appearance: none; /* Hides the slider so that custom slider can be made */
  width: 95%; /* Specific width is required for Firefox. */
  background: transparent; /* Otherwise white in Chrome */
  margin-bottom: 8px;
}

.hint a {
  font-size: 90%;
}

@media (max-width: 350px) {
  .hint a {
    font-size: 75%;
  }
}

input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
}

input[type=range]:focus {
  outline: none; /* Removes the blue border. You should probably do some kind of focus styling for accessibility reasons though. */
}

input[type=range]::-ms-track {
  width: 100%;
  cursor: pointer;

  /* Hides the slider so custom styles can be added */
  background: transparent;
  border-color: transparent;
  color: transparent;
}

/* Thumb */

/* Special styling for WebKit/Blink */
input[type=range]::-webkit-slider-thumb {
  -webkit-appearance: none;
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  margin-top: -6px; /* You need to specify a margin in Chrome, but in Firefox and IE it is automatic */
}

/* All the same stuff for Firefox */
input[type=range]::-moz-range-thumb {
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: steelblue;
  cursor: pointer;
  border: none;
}

/* All the same stuff for IE */
input[type=range]::-ms-thumb {
  height: 14px;
  width: 14px;
  border-radius: 50%;
  background: grey;
  cursor: pointer;
}

/* Track */

input[type=range]::-webkit-slider-runnable-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]:focus::-webkit-slider-runnable-track {
  background: rgba(0, 0, 0, 0.15);
}

input[type=range]::-moz-range-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}

input[type=range]::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: rgba(0, 0, 0, 0.1);
  border-radius: 2px;
  border: none;
}
input[type=range]::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-lower {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}
input[type=range]:focus::-ms-fill-upper {
  background: rgba(0, 0, 0, 0.1);
}

input[type="radio"] {
    background-color: steelblue;
}

#colab-hero-div { 
  grid-column: 1/3;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-top-width: 1px;
  border-top-style: solid;
  border-top-color: rgba(0, 0, 0, 0.1);
  padding-top: 15px;
}

#colab-hero {
  margin: auto;
  display: block;
  text-align: center;
  width: 200px;
  height: 16px;
}

</style>

<div class="l-body-outset grid" id="demo">

    <canvas id="demo-canvas" width="768" height="768"></canvas>

    <div id="demo-tip">
            <img id="pointer" src="images/pointer.svg">
            <div class="hint">
                Click or tap the image to erase the part of the pattern and see it regenerate.
                Double clicking places a new seed cell on the grid.                
            </div>
    </div>

    <div id="demo-controls">
        <div id="pattern-selector">
        </div>
        <div>
            <span id="play-pause">
                <svg class="icon" id="play"><use xlink:href="#playIcon"></use></svg>
                <svg class="icon" id="pause" style="display: none;"><use xlink:href="#pauseIcon"></use></svg>
            </span>
            <svg class="icon" id="reset"><use xlink:href="#resetIcon"></use></svg>
        </div>
        <div>
            Speed: <span id="speedLabel"></span><br>
            <input type="range" id="speed" min="-3" max="3" step="1" value="0"><br>
            <div id="status">
                Step <span id="stepCount"></span>
                (<span id="ips"></span> step/s)
            </div>
        </div>
        <div id="model-selector">
            Model type:<br>
            <input type="radio" name="model" id="ex1"> <label for="ex1">Growing</label><br>
            <input type="radio" name="model" id="ex2"> <label for="ex2">Persistent</label><br>
            <input type="radio" name="model" id="ex3"> <label for="ex3">Regenerating</label><br>        
        </div>
        <div style="white-space: nowrap;">
            Rotation <span id="rotationLabel"></span>&nbsp;<span class="hint"><a href="#experiment-4">[experiment 4]</a></span>
            <br>
            <input type="range" id="rotation" min="0" max="360" step="1" value="0"><br>
        </div>
        <div id='model-hints' class="hint">
            <span id="ex1-hint">
                <b>Growing</b> models were trained to generate patterns, 
                but don't know how to persist them. Some patterns explode, some decay,
                but some happen to be almost stable or even regenerate parts!
                <a href="#experiment-1">[experiment 1]</a>
            </span>
            <span id="ex2-hint">
                <b>Persistent</b> models are trained to make the pattern stay for a prolonged
                period of time. Interstingly, they often develop some regenerative
                capabilities without being explicitly instructed to do so
                <a href="#experiment-2">[experiment 2]</a>.
            </span>
            <span id="ex3-hint">
                <b>Regenerating</b> models were subject to pattern damages during
                training, so their regenerative capabilities are much stronger,
                especially in the central area. <a href="#experiment-3">[experiment 3]</a>

            </span>
        </div>
        <div id="colab-hero-div">
          <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb" class="colab-root" id="colab-hero">Try in a <span class="colab-span">Notebook</span></a>
        </div>
    </div>
</div>

<script type="module">
    import { createDemo } from './demo.js'
    createDemo('demo');
</script>

</d-title>

<d-byline></d-byline>


<d-article>
<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#model">Model</a></div>
    <div><a href="#experiment-1">Experiments</a></div>
    <ul>
      <li><a href="#experiment-1">Learning to Grow</a></li>
      <li><a href="#experiment-2">What persists, exists</a></li>
      <li><a href="#experiment-3">Learning to regenerate</a></li>
      <li><a href="#experiment-4">Rotating the perceptive field</a></li>
    </ul>
    <div><a href="#related-work">Related Work</a></div>
    <div><a href="#discussion">Discussion</a></div>
  </nav>
</d-contents>
<p>Most multicellular organisms begin their life as a single egg cell - a single cell whose progeny reliably self-assemble into highly complex anatomies with many organs and tissues in precisely the same arrangement each time. The ability to build their own bodies is probably the most fundamental skill every living creature possesses. Morphogenesis (the process of an organism’s shape development) is one of the most striking examples of a phenomenon called <i>self-organisation</i>. Cells, the tiny building blocks of bodies, communicate with their neighbors to decide the shape of organs and body plans, where to grow each organ, how to interconnect them, and when to eventually stop. Understanding the interplay of the emergence of complex outcomes from simple rules and homeostatic<d-footnote> Self-regulatory feedback loops trying maintain the body in a stable state or preserve its correct overall morphology under external perturbations</d-footnote> feedback loops is an active area of research <d-cite key="PezzuloGiovanniLevin2016,C5IB00221D"></d-cite>. What is clear is that evolution has learned to exploit the laws of physics and computation to implement the highly robust morphogenetic software that runs on genome-encoded cellular hardware.</p>

<p>This process is extremely robust to perturbations. Even when the organism is fully developed, some species still have the capability to repair damage - a process known as regeneration. Some creatures, such as salamanders, can fully regenerate vital organs, limbs, eyes, or even parts of the brain! Morphogenesis is a surprisingly adaptive process. Sometimes even a very atypical development process can result in a viable organism - for example, when an early mammalian embryo is cut in two, each half will form a complete individual - monozygotic twins! </p>

<p>The biggest puzzle in this field is the question of how the cell collective knows what to build and when to stop. The sciences of genomics and stem cell biology are only part of the puzzle, as they explain the distribution of specific components in each cell, and the establishment of different types of cells. While we know of many genes that are <i>required</i> for the process of regeneration, we still do not know the algorithm that is <i>sufficient</i> for cells to know how to build or remodel complex organs to a very specific anatomical end-goal. Thus, one major lynch-pin of future work in biomedicine is the discovery of the process by which large-scale anatomy is specified within cell collectives, and how we can rewrite this information to have rational control of growth and form. It is also becoming clear that the software of life possesses numerous modules or subroutines, such as “build an eye here”, which can be activated with simple signal triggers<d-cite key="Pai313"></d-cite>. Discovery of such subroutines and a mapping out of the developmental logic is a new field at the intersection of developmental biology and computer science. An important next step is to try to formulate computational models of this process, both to enrich the conceptual toolkit of biologists and to help translate the discoveries of biology into better robotics and computational technology.</p>

<p>Imagine if we could design systems of the same plasticity and robustness as biological life: structures and machines that could grow and repair themselves. Such technology would transform the current efforts in regenerative medicine, where scientists and clinicians seek to discover the inputs or stimuli that could cause cells in the body to build structures on demand as needed. To help crack the puzzle of the morphogenetic code, and also exploit the insights of biology to create self-repairing systems in real life, we try to replicate some of the desired properties in an <i>in silico</i> experiment.</p>

<h2 id='model'>Model</h2>
<p>Those in engineering disciplines and researchers often use many kinds of simulations incorporating local interaction, including systems of partial derivative equation (PDEs), particle systems, and various kinds of Cellular Automata (CA). We will focus on Cellular Automata models as a roadmap for the effort of identifying cell-level rules which give rise to complex, regenerative behavior of the collective. CAs typically consist of a grid of cells being iteratively updated, with the same set of rules being applied to each cell at every step. The new state of a cell depends only on the states of the few cells in its immediate neighborhood. Despite their apparent simplicity, CAs often demonstrate rich, interesting behaviours, and have a long history of being applied to modeling biological phenomena.</p>

<p>Let’s try to develop a cellular automata update rule that, starting from a single cell, will produce a predefined multicellular pattern on a 2D grid. This is our analogous toy model of organism development. To design the CA, we must specify the possible cell states, and their update function. Typical CA models represent cell states with a set of discrete values, although variants using vectors of continuous values exist. The use of continuous values has the virtue of allowing the update rule to be a differentiable function of the cell’s neighbourhood’s states. The rules that guide individual cell behavior based on the local environment are analogous to the low-level hardware specification encoded by the genome of an organism. Running our model for a set amount of steps from a starting configuration will reveal the patterning behavior that is enabled by such hardware.</p>

<p>So - what is so special about differentiable update rules? They will allow us to use the powerful language of loss functions to express our wishes, and the extensive existing machinery around gradient-based numerical optimization to fulfill them. The art of stacking together differentiable functions, and optimizing their parameters to perform various tasks has a long history. In recent years it has flourished under various names, such as (Deep) Neural Networks, Deep Learning or Differentiable Programming.</p>

<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:2000px;">
  <object data="figures/model.svg" type="image/svg+xml" style="width:100%"></object>
  <figcaption style="">
  A single update step of the model.
  </figcaption>
</figure></p>

<h3>Cell State</h3>
<p>We will represent each cell state as a vector of 16 real values (see the figure above). The first three channels represent the cell color visible to us (RGB). The target pattern has color channel values in range $[0.0, 1.0]$ and an $\alpha$ equal to 1.0 for foreground pixels, and 0.0 for background. </p>

<p>The alpha channel ($\alpha$) has a special meaning: it demarcates living cells, those belonging to the pattern being grown. In particular, cells having $\alpha > 0.1$ and their neighbors are considered “living”. Other cells are “dead” or empty and have their state vector values explicitly set to 0.0 at each time step. Thus cells with $\alpha > 0.1$ can be thought of as “mature”, while their neighbors with $\alpha \leq 0.1$ are “growing”, and can become mature if their alpha passes the 0.1 threshold.</p>

<figure>  <img src="figures/alive2.svg" style="width: 300px" />  
<figcaption> $\vec{state} \rightarrow 0.00$ when no neighbour with $\alpha > 0.10$ </figcaption></figure>


<p>Hidden channels don’t have a predefined meaning, and it’s up to the update rule to decide what to use them for. They can be interpreted as concentrations of some chemicals, electric potentials or some other signaling mechanism that are used by cells to orchestrate the growth. In terms of our biological analogy - all our cells share the same genome (update rule) and are only differentiated by the information encoded the chemical signalling they receive, emit, and store internally (their state vectors).</p>

<h3>Cellular Automaton rule</h3>
<p>Now it’s time to define the update rule. Our CA runs on a regular 2D grid of 16-dimensional vectors, essentially a 3D array of shape [height, width, 16]. We want to apply the same operation to each cell, and the result of this operation can only depend on the small (3x3) neighborhood of the cell. This is heavily reminiscent of the convolution operation, one of the cornerstones of signal processing and differential programming. Convolution is a linear operation, but it can be combined with other per-cell operations to produce a complex update rule, capable of learning the desired behaviour. Our cell update rule can be split into the following phases, applied in order:</p>

<p><strong>Perception.</strong> This step defines what each cell perceives of the environment surrounding it. We implement this via a 3x3 convolution with a fixed kernel. One may argue that defining this kernel is superfluous - after all we could simply have the cell learn the requisite perception kernel coefficients. Our choice of fixed operations are motivated by the fact that real life cells often rely only on chemical gradients to guide the organism development. Thus, we are using classical Sobel filters to estimate the partial derivatives of cell state channels in the $\vec{x}$ and $\vec{y}$ directions, forming a 2D gradient vector in each direction, for each state channel. We concatenate those gradients with the cells own states, forming a $16*2+16=48$ dimensional <i>perception vector</i>, or rather <i>percepted vector, </i>for each cell.</p>

<d-code block="" language="python">
<p>def perceive(state_grid):</p>
<p>  sobel_x = [[-1, 0, +1],</p>
<p>             [-2, 0, +2],</p>
<p>             [-1, 0, +1]]</p>
<p>  sobel_y = transpose(sobel_x)</p>
<p>  # Convolve sobel filters with states</p>
<p>  # in x, y and channel dimension.</p>
<p>  grad_x = conv2d(sobel_x, state_grid)</p>
<p>  grad_y = conv2d(sobel_y, state_grid)</p>
<p>  # Concatenate the cell's state channels, </p>
<p>  # the gradients of channels in x and </p>
<p>  # the gradient of channels in y. </p>
<p>  perception_grid = concat(</p>
<p>      state_grid, grad_x, grad_y,  axis=2)</p>
<p>  return perception_grid</p>
</d-code>

<p><strong>Update rule.</strong> Each cell now applies a series of operations to the perception vector, consisting of typical differentiable programming building blocks, such as 1x1-convolutions and ReLU nonlinearities, which we call the cell’s “update rule”. Recall that the update rule is learned, but every cell runs the same update rule. The network parametrizing this update rule consists of approximately 8,000 parameters. Inspired by residual neural networks, the update rule outputs an incremental update to the cell’s state, which applied to the cell before the next time step. The update rule is designed to exhibit “do-nothing” initial behaviour - implemented by initializing the weights of the final convolutional layer in the update rule with zero. We also forego applying a ReLU to the output of the last layer of the update rule as the incremental updates to the cell state must necessarily be able to both add or subtract from the state.</p>

<d-code block="" language="python">
<p>def update(perception_vector):</p>
<p>  # The following pseudocode operates on</p>
<p>  # a single cell’s perception vector. </p>
<p>  # Our reference implementation uses 1D</p>
<p>  # convolutions for performance reasons.</p>
<p>  x = dense(perception_vector, output_len=128)</p>
<p>  x = relu(x)</p>
<p>  ds = dense(x, output_len=16, weights_init=0.0)</p>
<p>  return ds</p>
</d-code>


<p><strong>Stochastic cell update.</strong> Typical cellular automata update all cells simultaneously. This implies the existence of a global clock, synchronizing all cells. Relying on global synchronisation is not something one expects from a self-organising system. We relax this requirement by assuming that each cell performs an update independently, waiting for a random time interval between updates. To model this behaviour we apply a random per-cell mask to update vectors, setting all update values to zero with some predefined probability (we use 0.5 during training). This operation can be also seen as an application of per-cell dropout to update vectors.</p>

<d-code block="" language="python">
<p>def stochastic_update(state_grid, ds_grid):</p>
<p>  # Zero out a random fraction of the updates.</p>
<p>  rand_mask = cast(random(64, 64) < 0.5, float32)</p>
<p>  ds_grid = ds_grid * rand_mask</p>
<p>  return state_grid + ds_grid</p>
</d-code>

<p><strong>Living cell masking.</strong> We want to model the growth process that starts with a single cell, and don’t want empty cells to participate in computations or carry any hidden state. We enforce this by explicitly setting all channels of empty cells to zeros. A cell is considered empty if there is no “mature” (alpha>0.1) cell in its 3x3 neightborhood.</p>

<d-code block="" language="python">
<p>def alive_masking(state_grid):</p>
<p>  # Take the alpha channel as the measure of "life".</p>
<p>  alive = max_pool(state_grid[:, :, 3], (3,3)) > 0.1</p>
<p>  state_grid = state_grid * cast(alive, float32)</p>
<p>  return state_grid</p>
</d-code>





<h2 id='experiment-1'>Experiment 1: Learning to Grow</h2>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:900px;">
  <object data="figures/training.svg" type="image/svg+xml" style="width:100%"></object>
  <figcaption>
  Training regime for learning a target pattern.</figcaption>
</figure></p>
<p>In our first experiment, we simply train the CA to achieve a target image after a random number of updates. This approach is quite naive and will run into issues. But the challenges it surfaces will help us refine future attempts.</p>

<p>We initialize the grid with zeros, except a single seed cell in the center, which will have all channels except RGB<d-footnote> We set RGB channels of the seed to zero because we want it to be visible on the white background.</d-footnote> set to one. Once the grid is initialized, we iteratively apply the update rule. We sample a random number of CA steps from the [64, 96]<d-footnote> This should be a sufficient number of steps to grow the pattern of the size we work with (40x40), even considering the stochastic nature of our update rule.</d-footnote> range for each training step, as we want the pattern to be stable across a number of iterations. At the last step we apply pixel-wise L2 loss between RGBA channels in the grid and the target pattern. This loss can be differentiably optimized<d-footnote> We observed training instabilities, that were manifesting themselves as sudden jumps of the loss value in the later stages of the training. We managed to mitigate them by applying per-variable L2 normalization to parameter gradients. This may have the effect similar to the weight normalization <d-cite key="Salimans2016WeightNA"></d-cite>. Other training parameters are available in the accompanying source code.</d-footnote> with respect to the update rule parameters by backpropagation-through-time, the standard method of training recurrent neural networks.</p>

<p>Once the optimisation converges, we can run simulations to see how our learned CAs grow patterns starting from the seed cell. Let’s see what happens when we run it for longer than the number of steps used during training. The animation below shows the behaviour of a few different models, trained to generate different emoji patterns.</p>


<p><figure>
    <video loop autoplay playsinline muted width="640px">
      <source src="figures/unstable.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  Many of the patterns exhibit instability for longer time periods. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure></p>

<p>We can see that different training runs can lead to models with drastically different long term behaviours. Some tend to die out, some don’t seem to know how to stop growing, but some happen to be almost stable! How can we steer the training towards producing persistent patterns all the time?</p>
<h2 id='experiment-2'>Experiment 2: What persists, exists</h2>
<p>One way of understanding why the previous experiment was unstable is to draw a parallel to dynamical systems. We can consider every cell to be a dynamical system, with each cell sharing the same dynamics, and all cells being locally coupled amongst themselves. When we train our cell update model we are adjusting these dynamics. Our goal is to find dynamics that satisfy a number of properties. Initially, we wanted the system to evolve from the seed pattern to the target pattern - a trajectory which we achieved in Experiment 1. Now, we want to avoid the instability we observed - which in our dynamical system metaphor consists of making the target pattern an attractor.</p>

<p>One strategy to achieve this is letting the CA iterate for much longer time and periodically applying the loss against the target, training the system by backpropagation through these longer time intervals. Intuitively we claim that with longer time intervals and several applications of loss, the model is more likely to create an attractor for the target shape, as we iteratively mold the dynamics to return to the target pattern from wherever the system has decided to venture. However, longer time periods substantially increase the training time and more importantly, the memory requirements, given that the entire episode’s intermediate activations must be stored in memory for a backwards-pass to occur.</p>

<p>Instead, we propose a “sample pool” based strategy to a similar effect. We define a pool of seed states to start the iterations from, initially filled with the single black pixel seed state. We then sample a batch from this pool which we use in our training step. To prevent the equivalent of “catastrophic forgetting” we replace one sample in this batch with the original, single-pixel seed state. After concluding the training step , we replace samples in the pool that were sampled for the batch with the output states from the training step over this batch. The animation below shows a random sample of the entries in the pool every 20 training steps.</p>
<d-code block="" language="python">
<p>def pool_training():</p>
<p>  # Set alpha and hidden channels to (1.0).</p>
<p>  seed = zeros(64, 64, 16)</p>
<p>  seed[64//2, 64//2, 3:] = 1.0</p>
<p>  target = targets['lizard']</p>
<p>  pool = [seed] * 1024</p>
<p>  for i in range(training_iterations):</p>
<p>    idxs, batch = pool.sample(32)</p>
<p>    # Sort by loss, descending.</p>
<p>    batch = sort_desc(batch, loss(batch))</p>
<p>    # Replace the highest-loss sample with the seed.</p>
<p>    batch[0] = seed</p>
<p>    # Perform training.</p>
<p>    outputs, loss = train(batch, target)</p>
<p>    # Place outputs back in the pool.</p>
<p>    pool[idxs] = outputs </p>
</d-code>

<p><figure>
     <video loop autoplay playsinline muted>
      <source src="figures/pool.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  A random sample of the patterns in the pool during training, sampled every 20 training steps. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure></p>

<p>Early on in the training process, the random dynamics in the system allow the model to end up in various incomplete and incorrect states. As these states are sampled from the pool, we refine the dynamics to be able to recover from such states. Finally, as the model becomes more robust at going from a seed state to the target state, the samples in the pool reflect this and are more likely to be very close to the target pattern, allowing the training to refine these almost completed patterns further. </p>

<p>Essentially, we use the previous final states as new starting points to force our CA to learn how to persist or even improve an already formed pattern, in addition to being able to grow it from a seed. This makes it possible to add a periodical loss for significantly longer time intervals than otherwise possible, encouraging the generation of an attractor as the target shape in our coupled system. We also noticed that reseeding the highest loss sample in the batch, instead of a random one, makes training more stable at the initial stages, as it helps to clean up the low quality states from the pool. </p>

<p>Here is what a typical training progress of a CA rule looks like. The cell rule learns to stabilize the pattern in parallel to refining its features.</p>

<p><figure>
    <video loop autoplay playsinline muted>
      <source src="figures/train_steps_damage_0.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  CA behaviour at training steps 100, 500, 1000, 4000. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure></p>

<h2 id='experiment-3'>Experiment 3: Learning to regenerate</h2>
<p>In addition to being able to grow their own bodies, living creatures are great at maintaining them. Not only does worn out skin get replaced with new skin, but very heavy damage to complex vital organs can be regenerated in some species. Is there a chance that some of the models we trained above have regenerative capabilities?</p>

<p><figure>
    <video loop autoplay playsinline muted>
      <source src="figures/regen1.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  Patterns exhibit some regenerative properties upon being damaged, but not full re-growth. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure></p>
<p>The animation above shows three different models trained using the same settings. We let each of the models develop a pattern over 100 steps, then damage the final state in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. For example “the lizard” develops quite strong regenerative capabilities, without being explicitly trained for it! </p>

<p>Since we trained our coupled system of cells to generate an attractor towards a target shape from a single cell, it was likely that these systems, once damaged, would generalize towards non-self-destructive reactions. That’s because the systems were trained to grow, stabilize, and never entirely self-destruct. Some of these systems might naturally gravitate towards regenerative capabilities, but nothing stops them from developing different behaviors such as explosive mitoses (uncontrolled growth), unresponsiveness to damage (overstabilization), or even self destruction, especially for the more severe types of damage.</p>

<p>If we want our model to show more consistent and accurate regenerative capabilities, we can try to increase the basin of attraction for our target pattern  - increase the space of cell configurations that naturally gravitate towards our target shape. We will do this by damaging a few pool-sampled states before each training step. The system now has to be capable of regenerating from states damaged by randomly placed erasing circles. Our hope is that this will generalize to regenerational capabilities from various types of damage.</p>

<p><figure>
    <video loop autoplay playsinline muted>
      <source src="figures/batches.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  Damaging samples in the pool encourages the learning of robust regenerative qualities. Row 1 are samples from the pool, Row 2 are their respective states after iterating the model.<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure></p>
<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (top-left-most in the above) with the seed state, and damage the three lowest-loss (top-right-most) states by setting a random circular region within the pattern to zeros. The bottom row shows states after iteration from the respective top-most starting state. As in Experiment 2, the resulting states get injected back into the pool.</p>

<p><figure>
    <video loop autoplay playsinline muted>
      <source src="figures/regen2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
Patterns exposed to damage during training exhibit astounding regenerative capabilities. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure>
</p>
<p>As we can see from the animation above, models that were exposed to damage during training are much more robust, including to types of damage not experienced in the training process (for instance rectangular damage as above).</p>

<h2 id='experiment-4'>Experiment 4: Rotating the perceptive field</h2>
<p>As previously described, we model the cell’s perception of its neighbouring cells by estimating the gradients of state channels in $\vec{x}$ and $\vec{y}$ using Sobel filters. A convenient analogy is that each agent has two sensors (chemosensory receptors, for instance) pointing in orthogonal directions that can sense the gradients in the concentration of certain chemicals along the axis of the sensor. What happens if we rotate those sensors? We can do this by rotating the Sobel kernels.</p>
<figure>$
\begin{bmatrix}
  K_x \\
  K_y
  \end{bmatrix}
  =
  \begin{bmatrix}
  \cos \theta &  -\sin \theta \\
  \sin \theta &  \cos \theta
  \end{bmatrix} 
  *
  \begin{bmatrix}
  Sobel_x \\
  Sobel_y
  \end{bmatrix}
$</figure>
<p>This simple modification of the perceptive field produces rotated versions of the pattern for an angle of choosing without retraining as seen below.</p>
<p><figure>
    <img src="figures/rotation.png" style="width: 448px">
  <figcaption>
Rotating the axis along which the perception step computes gradients brings about rotated versions of the pattern. <br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure>
</p>

<p>In a perfect world, not quantized by individual cells in a pixel-lattice, this would not be too surprising, as, after all, one would expect the perceived gradients in $\vec{x}$ and $\vec{y}$ to be invariant to the chosen angle - a simple change of frame of reference. However, it is important to note that things are not as simple in a pixel based model. Rotating pixel based graphics involves computing a mapping that’s not necessarily bijective and classically involves interpolating between pixels to achieve the desired result. This is because a single pixel, when rotated, will now likely overlap several pixels. The successful growth of patterns as above suggests a certain robustness to the underlying conditions outside of those experienced during training.</p>
<h2 id='related-work'>Related Work</h2>
<h3>CA and PDEs</h3>
<p>There exists an extensive body of literature that describes the various flavours of cellular automata and PDE systems, and their applications to modelling physical, biological or even social systems. Although it would be impossible to present a just overview of this field in a few lines, we will describe some prominent examples that inspired this work. Alan Turing introduced his famous Turing patterns back in 1952 <d-cite key="turing1990chemical"></d-cite>, suggesting how reaction-diffusion systems can be a valid model for chemical behaviors during morphogenesis. A particularly inspiring reaction-diffusion model that stood the test of time is the Gray-Scott model <d-cite key="Pearson189"></d-cite>, which shows an extreme variety of behaviors controlled by just a few variables.</p>

<p>Ever since von Neumann introduced CAs <d-cite key="10.5555/1102024"></d-cite> as models for self-replication they have captivated researchers’ minds, who observed extremely complex behaviours emerging from very simple rules. Likewise, the a broader audience outside of academia were seduced by CA’s life-like behaviours thanks to Conway’s Game of Life <d-cite key="10.2307/24927642"></d-cite>. Perhaps motivated in part by the proof that something as simple as the Rule 110 is Turing complete, Wolfram's “<i>A New Kind of Science”</i> <d-cite key="Wolfram2002ANK"></d-cite> asks for a paradigm shift centered around the extensive usage of elementary computer programs such as CA as tools for understanding the world.</p>

<p>More recently, several researchers generalized Conway’s Game of life to work on more continuous domains. We were particularly inspired by Rafler’s SmoothLife <d-cite key="rafler2011generalization"></d-cite> and Chan’s Lenia <d-cite key="Chan_2019, Reinke2019IntrinsicallyME"></d-cite>, the latter of which also discovers and classifies entire species of “lifeforms”.</p>

<p>A number of researchers have used evolutionary algorithms to find CA rules that reproduce predefined simple patterns <d-cite key="10.1007/978-3-642-19167-1_2, Nichele2018CANEATEC"></d-cite>. For example, J. Miller <d-cite key="Miller2004"></d-cite> proposed an experiment similar to ours, using evolutionary algorithms to design a CA rule that could build and regenerate the French flag, starting from a seed cell.</p>
<h3>Neural Networks and Self-Organisation</h3>
<p>The close relation between Convolutional Neural Networks and Cellular Automata has already been observed by a number of researchers <d-cite key="wulff1993learning, Gilpin2018CellularAA"></d-cite>. The connection is so strong it allowed us to build Neural CA models using components readily available in popular ML frameworks. Thus, using a different jargon, our Neural CA could potentially be named “Recurrent Residual Convolutional Networks with ‘per-pixel’ Dropout”.</p>

<p>The Neural GPU <d-cite key="Kaiser2015NeuralGPU, Freivalds2017ImprovingTN"></d-cite> offers a computational architecture very similar to ours, but applied in the context of learning multiplication and a sorting algorithm.</p>

<p>Looking more broadly, we think that the concept of self-organisation is finding its way into mainstream machine learning with popularisation of Graph Neural Network <d-cite key="wu2019comprehensive"></d-cite> models. Typically, GNNs run a repeated computation across vertices of a (possibly dynamic) graph. Vertices communicate locally through graph edges, and aggregate global information required to perform the task over multiple rounds of message exchanges, just as atoms can be thought of as communicating with each other to produce the emergent properties of a molecule <d-cite key="NIPS2015_5954"></d-cite>, or even points of a point cloud talk to their neighbors to figure out their global shape <d-cite key="Wang_2019"></d-cite>. </p>
<p>Self-organization also appeared in fascinating contemporary work using more traditional dynamic graph networks, where the authors evolved Self-Assembling Agents to solve a variety of virtual tasks <d-cite key="pathak19assemblies"></d-cite>.</p>
<h3>Swarm Robotics</h3>
<p>One of the most remarkable demonstrations of the power of self-organisation is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids <d-cite key="boids"></d-cite> simulated the flocking behaviour of birds with just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots with programs and test their collective behavior on physical agents, as demonstrated by work such as Mergeable Nervous Systems <d-cite key="mathews2017mergeable"></d-cite> and Kilobots <d-cite key="kilobots"></d-cite>. To the best of our knowledge, programs embedded into swarm robots are currently designed by humans. We hope our work can serve as an inspiration for the field and encourage the design of collective behaviors through differentiable modeling.</p>
<h2 id='discussion'>Discussion</h2>
<h3>Embryogenetic Modeling</h3>

<p><figure>
    <video loop autoplay playsinline muted>
      <source src="figures/planarian.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
Regeneration-capable 2-headed planarian, the creature that inspired this work <d-cite style="text-align: left;" key="WhatBodiesThink"></d-cite>
<br><br> <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>
  </figcaption>
</figure>
</p>
<p>This article describes a toy embryogenesis and regeneration model. This is a major direction for future work, with many applications in biology and beyond. In addition to the implications for understanding the evolution and control of regeneration, and harnessing this understanding for biomedical repair, there is the field of bioengineering. As the field transitions from synthetic biology of single cell collectives to a true synthetic morphology of novel living machines <d-cite key="Kriegman1853, Kamm2018"></d-cite>, it will be essential to develop strategies for programming system-level capabilities, such as anatomical homeostasis (regenerative repair). It has long been known that regenerative organisms can restore a specific anatomical pattern; however, more recently it’s been found that the target morphology is not hard coded by the DNA, but is maintained by a physiological circuit that stores a setpoint for this anatomical homeostasis <d-cite key="doi:10.1080/19420889.2016.1192733"></d-cite>. Techniques are now available for re-writing this setpoint, resulting for example <d-cite key="OVIEDO2010188,DURANT20172231"></d-cite> in 2-headed flatworms that, when cut into pieces in plain water (with no more manipulations) result in subsequent generations of 2-headed regenerated worms (as shown above). It is essential to begin to develop models of the computational processes that store the system-level target state for swarm behavior <d-cite key="doi:10.1162/isal_a_00043, PIETAK201852, doi:10.1162/isal_a_00041, doi:10.1162/isal_a_029"></d-cite>, so that efficient strategies can be developed for rationally editing this information structure, resulting in desired large-scale outcomes (thus defeating the inverse problem that holds back regenerative medicine and many other advances).</p>
<h3>Engineering and machine learning</h3>
<p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p>
</d-article>
<d-appendix>
<h3>Acknowledgments</h3>
<p>We would like to thank Blaise Aguera y Arcas for his support, as well as for teasing our work in his excellent 2019 talk at NeurIPS <d-cite key="SocialIntelligence"></d-cite>. We also thank Jyrki Alakuijala for his continuous support. We thank Damien Henry, Mark Sandler, Sean Silva and Bert Chan for their review of our early drafts, and Andrew Jackson for proofreading the text.</p>

<p>On the Distill side, we are especially grateful to Chris Olah for reviewing the article draft, insightful comments on text and diagrams, and general support of the publication.</p>
<h3>Author Contributions</h3>
<p><strong>Research:</strong> Alexander came up with the Self-Organising Asynchronous Neural Cellular Automata model and Ettore contributed to its design. Ettore designed and performed most of the experiments for this work. Alexander supervised the entire process and contributed extensively to the later stages of development by performing experiments and refining the model.</p>

<p>The idea of applying neural networks to understanding regeneration, and to designing self-organising systems, was proposed by Michael Levin in his email to Alexander, that was sent following the DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alexander in 2015. Alexander’s proposal of this model and this work were inspired by the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS 2018 as well as the subsequent email exchange between Alexander and Michael.</p>

<p><strong>Demos:</strong> Alexander created both the WebGL and the tf.js demo. Ettore contributed to the tf.js demo.</p>

<p><strong>Writing & Diagrams:</strong> Alexander outlined the structure of the article, and contributed to the content throughout. Ettore contributed to the content throughout. Eyvind drew all the diagrams, contributed to the content throughout, and wrote all of the pseudocode. Michael made extensive contributions to the article text, providing the biological context and motivation for this work.</p>
<h3>Implementation details</h3>
<p><strong>WebGL playground.</strong> Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p>

<p>While writing this article, we decided to see how far one can push the performance and portability of this interactive playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations<d-footnote> We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than the large ones. That’s why we use the non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</d-footnote> to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p>

<p>The quantization was largely an afterthought, and was not accounted for during training. That’s why there are slight differences in models’ behaviours between the online demo and the Python version. However, most of the CAs that we’ve trained managed to survive the somewhat draconic quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p>

<p><strong>Colaboratory Notebook.</strong> All of the experiments, images and videos in this article can be recreated using the single notebook referenced at the beginning of the article. Images have a “Recreate in Colab” button which brings you to the corresponding cell that generated the image. Our reference implementation of the Neural CA was written while striving to be as concise and simple as possible and thus foregoes many performance optimizations and tricks one could implement. For the core of the CA - the neural network parametrizing the update rule - the full code is contained in the tf.keras.Model NeuralCA class. Note that this network consists of just 8.3K parameters - minute by most standards and we suspect it could be minimized further employing pruning or other forms of compression. The update loop consists of a native python loop iteratively applying the aforementioned update function, and making use of various techniques we’ve described in the article, such as having a sample pool and applying damage to the starting seeds. The rest of the notebook consists of code to generate and visualize the various images and videos employed in this article, utilizing models pre-trained by us using this very same colab. These pre-trained models can be easily recreated in a matter of minutes with a current generation GPU or one provided for free in Colab.</p>

<d-footnote-list></d-footnote-list>
<d-citation-list></d-citation-list>
<d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>
</body>
